<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en" xml:lang="en">
<head>
<title>2 Interpretation as Probabilistic: Showing How a Text Is Made by Reading</title>
<meta content="text/html; charset=utf-8" http-equiv="default-style"/>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:ab716dc3-7c3d-4fc9-9c55-4cc6a0edf9ef" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter">
<div class="body">
<p class="sp"> </p>
<section aria-labelledby="ch2" epub:type="chapter" role="doc-chapter">
<header>
<h1 class="chapter-title" id="ch2"><span aria-label="43" id="pg_43" role="doc-pagebreak"/><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">2    Interpretation as Probabilistic: Showing How a Text Is Made by Reading</samp></h1>
</header>
<p class="noindent">As we have seen, knowledge and interpretation can be produced and represented in visual forms and formats. Graphical forms can structure arguments and express rhetorical positions. But can they be used to expose the workings of interpretation—show its models, assumptions, and operations? If so, through what means and with what benefits for humanities scholarship in its ethical as well as intellectual dimensions?</p>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Interpretation: A probabilistic approach</samp></h2>
<p class="noindent">If we are to model interpretation, we must understand what model <i>of</i> interpretation we are assuming. In the twentieth century, interpretation, or hermeneutics, went through what scholar Gerald Bruns has termed “a Copernican revolution”—by which he means the transformation of a user-independent model of textual meaning into one centered in individual understanding.<sup><a href="chapter_2.xhtml#chapter2-1" id="chapter2_1" role="doc-noteref">1</a></sup> Discussing Martin Heidegger and his profound influence, Bruns describes the shift from positing the site of meaning <i>within a text</i> to identifying it with the <i>act of reading</i> as a process of negotiation.<sup><a href="chapter_2.xhtml#chapter2-2" id="chapter2_2" role="doc-noteref">2</a></sup> For Bruns, the core concept within this tradition is that of <i>understanding</i>, which is to be conceived positionally <i>and</i> intellectually. <i>Understanding</i> emphasizes process, the state of being in a relation to a work (text, image, experience), rather than arriving at closure of meaning or sense.</p>
<p>The concept described by Bruns became the basis of critical theory in twentieth-century literary studies. A series of critical developments—reader-response, deconstruction, psychoanalytic theories of the subject, the play of work-text-word-trace, and other constructions of the dynamic production of text in the complex and situated act of reading—became the paradigms of interpretative, hermeneutic practice. The notion of an essential <span aria-label="44" id="pg_44" role="doc-pagebreak"/>meaning that could be divined from a text, to be extracted or exhumed, revealed or taken, was replaced with the idea that the reading subject occupied a position in relation to a work, and that the position was informed by the subject’s own lifeworld, knowledge, and conditions of identity across every possible and conceivable factor. Our positionedness and our subject identity were a complex of dynamic systems that factored into the relationship of reader to text. Identity politics of authorship and readership became essential features of critical work. The larger concept of decolonization of knowledge is the view that knowledge needs to be subject to critical assessment that takes into account its relation to the power systems in which it was produced. Decolonization thus embodies political impulses that have been fueled by historical insight and ethical reflection on traditions of cultural practice and their legacy, particularly in the west.</p>
<p>Decolonization draws on political critiques of unacknowledged bias as well as legacy assumptions structured by power relations of colonizer and colonized. These legacies have long histories and become almost invisible within the dominant (or hegemonic) practices of discourse formation. Subject positions imprinted through education and acculturation become blind to their own circumstances. To engage with understanding in the twenty-first century requires awareness of these formative conditions as part of our individual subject positionality. These tectonic shifts are marked in literary discourse and analysis as well as professional practices in information management and organization, but digital tools to inscribe the positionality of historically situated work (and subjects) have been slow to develop. Decolonization of knowledge is a strategic move and requires breaking the singularity of point of view often enacted by hegemonic discourse. Fracturing, faceting, and multiplying positions of cultural authority are crucial to this activity and builds on critical principles of hermeneutics, extending them into a dialogue with the political aspects of interpretation and/as knowledge.</p>
<p>The now-common conception of critical interpretation, or hermeneutics, acknowledges that a text does not transfer meaning in any mechanistic way, but it still stops short of the recognition that all acts of reading, productive and constitutive, are probabilistic—by which is meant that they are nondeterministic and selective. A text provokes a reading, allows it to come into being. Every reading is specific to its reading moment, it is an event generated between a reader and a text in a set of conditions never to be repeated. <span aria-label="45" id="pg_45" role="doc-pagebreak"/>The range of potential readings creates a field of possibilities, and every act of reading is an intervention in that field, a moment at which the generative potential of the text is intervened, flattened into a specific reading. A normative distribution applies here, with the bulk of readings of a given text clustered together and outliers stretching the field. But that hardly matters, because even if the readings aligned, were so close as to appear almost the same, minute examination would reveal the inscriptional specificity of each one. No reading is ever the same as another, and no text is ever equivalent to the interpretation it provokes. Inscriptional specificity, as described in the previous section, applies to experience as well as representation.</p>
<p>We arrive in the early twenty-first century, therefore, with a clear recognition that knowledge, as well as processes of knowing through interpretative work, is situated, partial, historical, and cultural in its formation. Individual readers engage in textual interpretations in accord with their training and background as surely as the texts are also the expression of specific circumstances of production. This recognition does not, by any means, reduce hermeneutics to demographics, nor texts to symptomatic expressions; quite the contrary, it simply insists that the start point for the production of a work through an interpretative (hermeneutic) encounter is always a complex act of codependence.</p>
<p>As my former collaborator Jerome McGann put it in 2001: “a literary work codes a set of instructions for how it should be read. Unlike machine and program codes, however, these codes decipher to an indeterminate number of precise outcomes. They represent exactly what Jarry called ‘a science of exceptions.’”<sup><a href="chapter_2.xhtml#chapter2-3" id="chapter2_3" role="doc-noteref">3</a></sup> McGann drew on the work of late nineteenth-century poet Alfred Jarry, who defended statistical anomalies as the foundation of the field he called ’pataphysics.<sup><a href="chapter_2.xhtml#chapter2-4" id="chapter2_4" role="doc-noteref">4</a></sup> In the context of SpecLab, the theoretical digital research group McGann and I conducted in the 2000s, I coined the additional term “patacriticism” to gesture toward the use of Jarry’s concepts within our research.<sup><a href="chapter_2.xhtml#chapter2-5" id="chapter2_5" role="doc-noteref">5</a></sup> The notion was that statistical norms and procedures of interpretation had to be balanced with other probabilistic considerations, among them the recognition that outliers were to be taken as seriously as acts of “deformance” (deliberate distortion or transformation of a work), along with other interpretative techniques.</p>
<p>Our approach at SpecLab focused on the probabilistic character of reading within the design of Ivanhoe, a game of interpretation designed explicitly within the digital humanities, on which more will be said below. The <span aria-label="46" id="pg_46" role="doc-pagebreak"/>challenge we grappled with still remains: to model the basic principles of a probabilistic approach and give visual expression to its constructedness, partial understanding, and the situated condition of interpretation. In this section, several concrete suggestions will be discussed for using graphical means to (1) break the apparent singularity of statements, (2) introduce comparative methods that expose assumptions, (3) demonstrate the madeness of information and graphical expressions, and (4) embody interpretative principles in the design of platforms. The goals of these interventions are value-driven (ideological) and ethical as well as intellectual.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Digital humanities and probability</samp></h2>
<p class="noindent">Digital tools penetrated the humanities with increasing impact from the 1970s and ’80s onward.<sup><a href="chapter_2.xhtml#chapter2-6" id="chapter2_6" role="doc-noteref">6</a></sup> They were part of the world of information professionals much earlier in data processing and records management, and some of the early repository building and text markup tools crossed between the management and research domains. However, the tasks taken up in digitization not only did <i>not</i> challenge computational techniques, but submitted to them, albeit often with considerable reflection and discussion, much of which ended with a shrug and then submission to the demands of computational processes at the price of theoretical concerns. Text analysis made use of searching, matching, counting, sorting—all methods that rely on using nonambiguous instances of discrete and identifiable strings of information coded in digital form. This kind of automation depended on principles that seemed to be nonhumanistic. The creation of a concordance, as in the oft-cited case of Father Roberto Busa’s pioneering work in the 1950s on Thomas Aquinas, does not, on the surface, appear to require interpretation. The identification and counting of particular words seems simple. But a word does not have a single value or meaning. Even identifying a term requires selection—should it be just the word, its variants, the word with one or two words collocated around it, and so on. That such work is necessarily interpretative, that it actually depends upon a <i>model</i> of the work being done, was a point carefully left aside. The results of the automation and insights it provoked were a tool for analysis of Aquinas’s vocabulary, but still had to be interpreted. But the work and its methods did not produce a challenge to computation or call for extending its capacities to engage with ambiguities and contradictions.</p>
<p><span aria-label="47" id="pg_47" role="doc-pagebreak"/>By the late 1980s and early 1990s, humanists were engaged in critical editing and repository building using digital tools and then networked environments. Metadata (information about data) and markup languages (formal tag sets inserted into a text for purposes of interpretation) are both highly structured forms with professional communities and standards. Working with them forced a dialogue between traditional humanities and the formalized information structures of content types and data. The mantra of the period—that the engagement with digital tools forced humanists to make explicit many of the assumptions in their work that had long remained implicit—was complemented by another observation: that humanists started the design of their digital projects as relativists (filled with the lessons of deconstruction, critical theory, poststructuralism) and then gave in to the pragmatic exigencies of positivism just to make things operate. Introducing ambiguity, for instance, at the level of markup languages, metadata, or other data structures, simply did not work. Or so we were told and came to believe.</p>
<p>Formal languages have the capacity to encode complex and ambiguous multiplicities of meaning. The problem of encoding ambiguity, for instance, is not technical. But at the level where project design was being governed by protocols that required explicit and unambiguous terms of operation, ambiguity could not be permitted. No technical obstacle prevents overlapping hierarchies in markup provided each interpretative frame is stored on an independent layer, for instance—a simple solution to a complex problem.<sup><a href="chapter_2.xhtml#chapter2-7" id="chapter2_7" role="doc-noteref">7</a></sup> But overlapping hierarchies, a much-discussed problem in the 1990s use of markup code, could not be addressed easily within the then-existing tools. More sophisticated models and tools have been developed, but these have not resulted in the creation of new data structures, just means of storing and processing markup. The challenge of modeling tools from humanities principles has been engaged in a few instances (such as NodeGoat and CATMA, both tools meant to support more intuitive and flexible interpretation in digital environments). But for the most part, researchers in the digital humanities have proceeded to develop standard platforms (or adopt them) without pushing humanistic concerns into the computational operations. Even basic techniques for being able to put multiple values into a field or create a metric based on affective values have not been designed into digital humanities systems. Proposals for this kind of modification are systematically integrated into the projects described in the final chapter of this book.</p>
<p><span aria-label="48" id="pg_48" role="doc-pagebreak"/>In asking how digital techniques would change, therefore, to embody these interpretative dimensions, I assert that humanities approaches would proceed from a number of specific principles. The first is that interpretation is performative, not mechanistic—in other words, as noted above, a text is not self-identical, each reading produces a text; discourses construct their objects; texts (in the broad sense of linguistic, visual, acoustic, filmic, dramatic, even architectural or site-specific works) are not static objects but encoded provocations for reading. Finding ways of showing these principles informed our discussions at SpecLab, in particular in trying to design the Ivanhoe platform. But the fuller project of showing interpretation, modeling it, making a composition space in which ambiguity and contradiction could coexist, where the non-self-identicality of objects could be made evident within their codependent relation to the social fields of production from which they spring (a relation premised on the constructedness of their identity, rather than the relation of an “object” to a “context”) remains unrealized. The caveat that an instantiation of an interpretative act might reify it in ways that create the illusion of fixity does not obviate the need to push for these experiments with humanistic precepts. Especially since the earlier phases of critical editing, repository building, and markup have been augmented by analytic processes of all kinds, with their dependence on visualizations. The tools of visualization, in particular, depend on statistical and quantitative methods from other disciplines. Most significantly, they depend upon explicit parameterization. But almost no humanistic document or discourse lends itself to such parameterization. For instance, how do we date documents—by year of conception, execution, publication, reception? Or how do we characterize language as gendered? What are the moving targets of meaning and semantic value as the use of a word or phrase shifts across contexts and populations? How do you identify a sentiment consistently? The basic impossibility of creating metrics appropriate to humanistic work motivated my argument about the need to distinguish the constructed-ness of <i>capta</i> as an alternative to assumptions about the given-ness of <i>data.</i><sup><a href="chapter_2.xhtml#chapter2-8" id="chapter2_8" role="doc-noteref">8</a></sup></p>
<p>The argument against quantitative reductiveness is not a dismissal of statistical methods, quite the contrary. One of the problems of digital humanities research has been the use of counting methods (how many instances of X appear in a corpus) rather than statistical ones (given that X appears, how likely is it that it will appear again?) in shaping projects. Statisticians are concerned with probabilities, not certainties. They do not merely count things; <span aria-label="49" id="pg_49" role="doc-pagebreak"/>they model conditions and possible outcomes. In an interpretative model, what disposes a reader to assign value to a word or image? Data mining in the humanities has largely depended on automated calculations and simple counting. Statistical modeling has factored less in the analytic toolkit of humanists than in social sciences. Stylometrics, attribution studies, natural language processing, and other higher level analyses have long made use of statistical techniques.<sup><a href="chapter_2.xhtml#chapter2-9" id="chapter2_9" role="doc-noteref">9</a></sup> But even when these are used, the design of graphic conventions for showing ambiguous, contradictory, or partial knowledge is in early stages, where it exists at all. Work in archaeological reconstruction, where various models have to be extrapolated from partial and fragmentary remains, has created spectral palimpsests to portray degrees of certainty. But even highly speculative economic, climate, or population models have not pushed the development of graphical methods that can fully serve to present their basic probabilistic premises. Humanists have not stepped in to fill the breach, even though the interpretative methods that lead their work require it. We need to develop an inventory of techniques for indicating, for instance, the distinction between what is known and what is projected, pronouncements linked to evidence and speculative rhetoric. How can we show how we think—and how the objects we think about are constructed in the process?</p>
<p>Because humanistic theory provides ways of thinking differently, along lines of interpretative knowing—partial, situated, enunciative (speaking and spoken positions), subjective, and performative—our challenge is to take up these theoretical principles and engage them in humanistic methods of production, <i>and the production of these methods</i>. We need to create ways of doing our work that allow us to display its models—not merely modeling a text as if it were a given, or as if it were self-evident, but modeling our reading and interpretation of it.</p>
<p>Experimental approaches to the use of visualization take two varied approaches—data display and modeling. Experiments in data display focus on improving discovery tools, using ways to filter data in various ways that can be made legible by the visualization. The distinction between display and modeling also reinforces the differences between concepts of data and capta.<sup><a href="chapter_2.xhtml#chapter2-10" id="chapter2_10" role="doc-noteref">10</a></sup> In these circumstances, data are considered objective “information,” but in fact, data is information that is captured because it fits the model of what is being measured or parameterized. In other words, all data is actually capta. The data does not exist independently, but is captured as <span aria-label="50" id="pg_50" role="doc-pagebreak"/>the result of the parameters of the search. The distinction between data display and modeling interpretation is more difficult to maintain in practice than to define in theory. Because data are based on models, we tend to see what we look for in accord with interpretative agendas. Our models continually change, however, and are often iterative in a hermeneutic practice. For instance, the plausibility of dinosaur imagery in popular imagination has a direct relation to the use of increasingly photographic properties of realism. These depictions make the dinosaurs believable because they conform to visual codes that seem linked to the observation of real objects—but then our idea of what a dinosaur was becomes imprinted by those visual images so we look for evidence to match these. Scientific observation follows constructs and concepts. The new vogue for feathered dinosaurs is the effect of a new model based on interpretation of new and older evidence combined. This is usual. Similarly, terms like “terrorism” and “hacktivism” emerge and become defined through use, then seem to refer to an actual category in the world, not merely a term in discourse. Knowledge formation is always a shifting realm of concepts, themselves part of a larger stochastic field of cultural processes. Disciplinary and cultural shifts in knowledge are marked in and effected by such changes. The notion that knowledge is a process of understanding, not an apprehension of things in the world, arises from a constructivist approach that engages probabilistic models. Theories of subjectivity form a crucial component in our current conception of interpretative knowledge as understanding. Without it, knowledge production and representation in the humanities—in any field—will remain locked into mechanistic models of thought in which an image/text is “out there” and an eye brings it “into” the mind. Approaches to knowledge that draw on complexity theory extend models of emergent behavior in agent-based projections and systems that model nonlinear processes. These systems could be used to model graphical expressions of interpretative capta as something that arises from a process rather than existing in advance.</p>
<p>Subjectivity is a structuring principle, not just an inflection. Used in the theoretical sense developed in linguistics and psychoanalysis, subjectivity refers to the place of the observer within phenomena. Subjectivity is conceived as a dynamic, codependent system of relations between the observer and the observed. But though this works at the level of individuals, subjectivity is always situated in social space. To cite McGann again, “Stanley Fish’s concept of an interpretative community is a device for measuring the <span aria-label="51" id="pg_51" role="doc-pagebreak"/>probability function of different interpretative acts. How those probabilities emerge—how certain acts of interpretation gain authority—is a problem that will have to be addressed by studying the normative dimensions of textual fields.”<sup><a href="chapter_2.xhtml#chapter2-11" id="chapter2_11" role="doc-noteref">11</a></sup></p>
<p>Early in the twentieth century, physicists recognized that empirically based concepts of natural laws grounded in mechanistic, Newtonian models were not sufficient to explain many physical phenomena.<sup><a href="chapter_2.xhtml#chapter2-12" id="chapter2_12" role="doc-noteref">12</a></sup> Quantum physics and principles of uncertainty characterized a radical change in the ways the observer and observed phenomena are understood, collapsing the two into a dynamic system of codependencies. Rather than imagine discrete phenomena available for independent observation, or the subject-object relationship as a dialogue between two independent entities, the quantum theorist suggests that phenomena arise when an observer intervenes in a field of potentialities. Before that intervention, phenomena may exist in an indeterminate condition, between two states. The intervention forces a resolution (this is the famous experiment known as Schr<span class="dcrit">ö</span>dinger’s cat, named for the physicist who invented it). Probabilistic methods based on the same principle belong squarely within the realm of humanistic methods, but they have rarely been invoked.</p>
<p>Theories of radical constructivism (a theory of knowledge that posits a codependence between phenomena and knowledge) and cognitive studies provide additional disciplinary frameworks. These stress dynamic, relational, systems-based, emergent concepts of knowledge that are as far from na<span class="dcrit">ï</span>ve empiricism or behavioral psychology as Heisenberg’s uncertainty is from Cartesian rationalism and Newtonian physics.<sup><a href="chapter_2.xhtml#chapter2-13" id="chapter2_13" role="doc-noteref">13</a></sup> McGann sketched this theoretical intersection in his 2001 “Texts in N-Dimensions” composed at the time we were designing Ivanhoe. His approach to texts combined theories of knowledge and cognition to make use of quantum theory as part of interpretative practice and the study of texts. This probabilistic approach to interpretation extended reader-response theory into a dialogue with indeterminacy as conceived in early twentieth-century theoretical physics. Though our work in the humanities was being done a century later, it still provoked skepticism.</p>
<p>However, with this framework in mind, we can move on to describe ways to use graphical forms for the self-conscious creation of knowledge. In other words, we can outline ways to use visualization to create, show, and compare probabilistic hermeneutic approaches. Our first area of focus <span aria-label="52" id="pg_52" role="doc-pagebreak"/>will be on the principle of constructedness that emerges from the distinction between data and capta and the ways visualization methods can be used comparatively to expose the interpretative dimensions of quantification (production of numerical values) and parameterization (specification of values to be captured). This discussion lays the foundation for attending to the problems of argument structures in a discourse field, the partial and situated character of interpretation, and basic operations of hermeneutic work.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Constructedness: Visuality and probability</samp></h2>
<p class="noindent">Each form of expression, and each medium, has its own properties, and these require specific interpretative approaches. Architecture cannot be analyzed in the same way as music, for instance, no matter how many comparisons can be made, and likewise, a poem cannot be interpreted through attention to the same properties one looks at in a dance or an image. Visual works (and film, performance, and other materials) lend themselves to some of the same critical practices as text, but only in the sense that all aesthetic objects are engaged in a similarly probabilistic manner. The specificity of visuality comes into play in interpreting visual work because of the graphic properties of composition, iconography, style—and the instability of visual signs. The training to read the codes of visual images is distinct from the training to read texts, and the histories of art history, visual communication, and web design are full of methods appropriate to understanding visual symbols and structure. Similarly, specialized training is needed for reading aerial photographs in military surveillance, sonograms and MRIs in medical research, or any other graphical expression used within professional domains. As per the earlier discussion, no “innocent eye” exists, and no images can be apprehended directly without some amount of expertise. Human beings in varied cultural circumstances may not even recognize an image as a visual representation—the basic category has to be learned before it can be understood. The interpretative engagement with visual forms is therefore as highly mediated a reading practice as that of texts, formulae, musical scores or any other encoded artifact of human expression. Critical intervention in a visual discourse field is at least as complex as it is within the nuanced subtleties of a textual one.</p>
<p>But in this context, our focus is on how visual formats can be used to <i>show</i> models of interpretation in three areas of activity: data/capta <span aria-label="53" id="pg_53" role="doc-pagebreak"/>production, situated authorship, and argument structures. Can we expose <i>models</i> of interpretation in these activities and give them graphical form as a means of apprehending specific features and properties?</p>
<p>To begin, we need to reiterate the distinction between <i>data</i> and <i>capta</i>. <i>Capta</i> is created from phenomena, abstracted through parameterization into what becomes reified as <i>data</i>, and then engages larger structures of interpretative arguments constructed on and through a rich discourse field. One of the primary ethical issues in such practices is to dispel the assumed neutrality of data production. Can visual means be used in part to locate the situated authorship of data, the position from which it is created, spoken, and then used as if it were value-free? A crucial task is to expose the myth of data as “given” and observer-independent. The fundamental interpretative act of data production is often concealed, in part because no conventions exist to display its processes, and in part because the intellectual task is often discounted. A second major issue is to demonstrate the situatedness of authorship and readership. This is fundamental to humanistic research since it locates scholarship and argument within communities of practice and individual circumstances. Finally, how can we call attention to argument structures and the way they make use of linking, organization, and analysis of evidence in the form of documents, artifacts, materials, sites. The extended use of analytic derivatives and surrogates adds new dimensions to historical and cultural values within argument structures, and these should be signaled as well.</p>
<p>To show the process of <i>capta</i> creation means creating a series of discrete steps for depicting the way statistical operations shape the production of data from phenomena. What, for instance, is the initial material from which data are produced? How is the sample size represented in relation to that original phenomenon? How can specific decisions about parameterization (specification of what can be counted) be revealed? Two approaches to answering this question present themselves immediately. One is to show the decisions built into the lifecycle of data production, the other is to show the nonsingularity of any presentation of <i>data</i> as if it were simply a statement. Both of these approaches can muster graphical means to their cause.</p>
<p>While documentation is essential for showing the interpretative decisions that go into modeling data, a graphical contrast that exposes how the model would change if any single parameter were altered makes the existence of models dramatically clear. For example, let’s set the following <span aria-label="54" id="pg_54" role="doc-pagebreak"/>problem: For each value or parameter used to create a sample set of data, show that a different sample would be produced by altering any single factor in the model. This simple move immediately shows that the data produced by any model is only an expression of that model and that the visualization is of the data model, not the phenomenon from which it was extracted. The outcomes can be compared graphically. The contrast of charted information, even in standard formats, makes a striking intervention into the declarative singularity, the appearance of simply stating “what is,” of the graphic statement. Singularity is the quality that a statement has when it appears to be indisputable, a statement that merely seems to declare a self-evident fact or observation. One potent critical tool for deconstructing the singularity and invisible authority of any statement is contrast. Comparison breaks the hold of any appearance of singularity of declarative statements in intellectual work and knowledge systems. As soon as such a statement is expressed graphically and the visualization is relativized, shown to be <i>a</i> statement about the “data” and not an uninflected statement of fact, then the recognition of the constructedness of the data, its visualization, and the model on which it is formed are all evident. (For instance, imagine changing the scale on a vertical axis in a graph to exaggerate the presentation of difference in value. Such an act demonstrates that the value is encoded in the graphical presentation, not independent of it. The graph is not just “showing” the value, it is making a graphic argument about value in the way it presents the information.) Instead of the usual “this is the data” statement, a comparison of one outcome with another (e.g., stretched vs. compressed scales) makes the argument that “these are propositions about how data might be abstracted and rendered.” In addition, the purpose and motivation of the data model should be revealed in documentation—for whom and in whose interests is the model working? Who paid for the work? Why? These are factors that do not rely on visualization, but are crucial in the reading of the outcome and the visual presentation.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Distinguishing data and capta</samp></h2>
<p class="noindent">To overturn the assumptions that structure conventions acquired from other domains requires that we reexamine the intellectual foundations of digital humanities, putting techniques of graphical display on a foundation that is humanistic at its base. This means first and foremost that we <span aria-label="55" id="pg_55" role="doc-pagebreak"/>reconceive all data as capta. Differences in the etymological roots of the terms data and capta make the distinction between constructivist and realist approaches clear. Capta is “taken” actively while data is assumed to be a “given,” able to be recorded and observed. From this distinction, a world of differences arises. Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of preexisting fact. My distinction between data and capta is not a covert suggestion that the humanities and sciences are locked into intellectual opposition, or that only the humanists recognize that intellectual disciplines create the objects of their inquiry. Any self-conscious historian of science or clinical researcher in the natural or social sciences insists the same is true for their work. Statisticians are savvy about their methods and about the way choices made in their processes shape arguments and outcomes. Social scientists may divide between realist and constructivist foundations for their research, but none are na<span class="dcrit">ï</span>ve when it comes to the rhetorical character of statistics as a tool.</p>
<p>Making a distinction between single and comparative statements using graphical expressions is one way to expose the probabilistic character of interpretation. This shows the nonequivalence of different statements at the level of data understood as capta. Showing decisions about parameterization or the shape of data as capta would not guarantee that one or the other had a greater claim to accuracy. Instead, the move would show that each are acts of probabilistic interpretation given graphical expression. The point of such comparisons is to show the constructedness of data, understood as capta. No statement, taken as a single, self-evident expression of data, or of features of phenomena expressed as data, can provide this reflection on the process of its production. Relativizing a statement through contrast and showing this contrast graphically calls attention to the <i>modeled</i> nature of the information and its expression.</p>
<p>Take a simple example. Imagine that the occupancy of classrooms needs to be calculated for purposes of space resource assessment. Now, consider the fundamental decisions about parameterization: when do I take the sample, how frequently, at what point in what cycle of periods? Every day at noon? Every night at eight pm? For how long should the period of counting last? Should the sample be taken once at three pm and once at nine am and only on Mondays? What about Sundays? During the entire calendar year? <span aria-label="56" id="pg_56" role="doc-pagebreak"/>Or in July? Mid-December? Anyone doing the least bit of statistical research understands the implications of these decisions. No single answer to these questions suffices, but every answer embodies the particulars according to which the “data” (again, my preferred term is <i>capta</i>) are produced. Making a series of contrasting calculations and showing them graphically would perform two important intellectual acts—of exposing the decision process of capta production and of demonstrating the rhetorical force of graphical expressions as propositional argument structures rather than as declarative statements. The charts become “what if” statements rather than “this is” statements, and the user-dependent conditions of knowledge production become inscribed in the process of presentation.</p>
<p>The model of interpretation does not simply show the single chart or graph of data points. A chart may express a model, but it is not the model, which is a schematic template that can be used repeatedly. The model consists of decisions shaping the particular parameters or template from which the data or capta arise. A simple but critically revealing exercise to do with a data set is simply to send it through a series of standard graphical expressions—turning it into a line graph, bar chart, pie chart, and/or tree diagram without altering the data set. This makes the graphical properties of standard conventions legible—in part because they are often nonsensical expressions of the information (percentages of opinions held in a population, for instance, do not belong on a continuous graph, with its implication of change over time, any more than marked increases in growth of a species in an area belongs in a pie chart with its static structure designed to show percentages of a whole). These exercises expose the rhetoric of conventions, but not the deeper issue of the way an interpretation depends upon a model.</p>
<p>Interpretative models linked to analytics can be exposed through the basic contrast of alternatives suggested in the two sample exercises just described. But intellectual models are more amorphous and elusive. Topic modeling, data mining, text analysis, and network production, as well as simple collocation and word frequency, are <i>modeled,</i> not simply performed. So is markup, and any kind of critical analysis, historical work, or cultural criticism. Providing a way to see the terms of the model, again by contrast, is an essential move.</p>
<p>The modeling of gender provides a much-studied example. Such work has also been thoroughly critiqued by Laura Mandell. She has systematically analyzed the ways gender modeling in digital humanities bears the <span aria-label="57" id="pg_57" role="doc-pagebreak"/>imprint of unexamined assumptions and their implications that can be exposed <i>as a model.</i><sup><a href="chapter_2.xhtml#chapter2-14" id="chapter2_14" role="doc-noteref">14</a></sup> What are the specific features of language that are being identified as gendered; how do they vary across texts, conditions, periods, and authors; and what terms of inclusion and exclusion, selection and variation, flexibility and mutability, are being used to create statistical analyses of a text or corpus? Asking these questions and then varying the models to demonstrate precisely that they <i>are</i> models, makes an argument that no single statement or presentation can. Breaking the claim to authority embodied in the single statement is a crucial means of demonstrating the constructedness of interpretative work. But also, this points to the situatedness of intellectual work within the varying demographics (population profiles structured according to age, income, race, or other features) and communities of practice, whose interests are also at work in shaping data models and presenting their outcomes.</p>
<p>Graphical formats can work effectively to show and contrast models for the same reasons that they work to expose patterns in data—they are legible and succinct and provide a gestalt overview of large-scale or complex patterns. Only documentation can provide the fuller critical material about communities of research and practice but multiple, layered, faceted visualizations are a tool of critical hermeneutics. They show that every presentation is the outcome of a probabilistic inquiry, a “what if” proposition, not a “what is” statement. This is a crucial intervention in the singularity of authoritative statements of “mere” fact.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Discourse fields</samp></h2>
<p class="noindent">With this discussion in place, we have a positive answer to the question of whether visualization methods in digital humanities can be used to show the critical, probabilistic, and multidimensional features of interpretation. The relation between interpretation and any field of evidence is always partial and incomplete, <i>a reading through</i> rather than <i>a reproduction of</i> the materials from which an argument is drawn. We need conventions for showing the features of observer-dependent knowledge production that are not part of the history of techniques used in empirical observation, with its assumptions about certainty, disambiguation, and repeatability.</p>
<p>One of the early tenets of belief in digital humanities, associated with the tasks of textual markup, was that the engagement with computational <span aria-label="58" id="pg_58" role="doc-pagebreak"/>techniques required that humanists make explicit many aspects of their work that had always been implicit—such as the categories on which distinctions and judgments were made. Some of these were relatively straightforward: Does a fragment of text describe an action or an intention? A character’s mood or a metaphor of natural forces? A historical event or a personal one? These kinds of decisions govern much of the interpretative activity of textual analysis and markup. When they have to be aligned with hard categories and decisions that assign a value to a word, phrase, or text, then the complexity of embedded and associative properties of language becomes evident. Resolving these dilemmas by forcing explicitness was an early solution. Creating means of rendering the ambiguities clearly and legibly would be a step forward in aligning the digital tools with humanistic methods. The ways the cultural authority of certainty asserts itself, or conceals itself, through digital practices is highly charged. Finding alternatives that counter the certainty of computation with the generative dialogue of interpretation is crucial.</p>
<p>The legacy of critical hermeneutics shifts in the encounter with digital methods. Humanistic approaches force us rethink the compromises these methods brought about through “datafication” of the cultural materials that are central to the humanities and the cultural record. The question arises as to whether aesthetic works require consideration distinct from that of other kinds of works (administrative, fiscal, medical, commercial, etc.) or whether the encounter with these works demonstrates principles useful to all cultural objects. Does a plumber’s manual, a medical handbook, an automobile, a subway system switching diagram, a work of technological engineering require the same critical approach as a poetic text? The short answer is simple: not always. (If I am looking through a plumbing catalogue to find a replacement part for a broken toilet, I may not want to engage in critical, interpretative, and probabilistic processes. I might just want to find the right part.) Still, the relevance of critical approaches to all aspects of human expression is one lesson bequeathed to us by anthropology and cultural studies. Of course, differentiating among kinds of texts and their potentiality (and the reader’s purpose at any moment) is an important aspect of critical engagement.</p>
<p>But our concern is with the needs of the humanities and the validity of humanistic methods within the growing realm of digital work (work remediated through digitized forms, formats, files, processes, and expressions, <span aria-label="59" id="pg_59" role="doc-pagebreak"/>usually within networked environments). In this arena, the struggle is not about what objects should come under examination or consideration, but how the critical approach should be formulated to incorporate hermeneutic traditions. Stealth positivism is rampant in digital work, and the unexamined consequences of its use are many. Chief among these, and perhaps the core issue of ceding intellectual ground from the humanities to the methodological instrumentality of empirical (seeming) methods, is the abandonment of critical hermeneutics as if it were inconvenient baggage, an unnecessary impediment to the computational techniques we press into service for our tasks. But why? Without any doubt, computational techniques are themselves hermeneutic, and the possibilities for pushing critical approaches beyond the limits of reading practices formulated within the traditions of twentieth-century philosophy and its paradigm of the hermeneutic circle and into a probabilistic, processual, and constructivist model have never been richer, more possible, and even more pressing.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Ivanhoe: Designing a game of interpretation</samp></h2>
<p class="noindent">In the early 2000s, in the context of SpecLab at the University of Virginia, McGann and I led a team that incorporated many theoretical principles into the design of Ivanhoe.<sup><a href="chapter_2.xhtml#chapter2-15" id="chapter2_15" role="doc-noteref">15</a></sup> Some of these had already featured in temporal modeling design. Others were implemented in the working prototype. Among these was acknowledging the positionality of historical subjects as intrinsic (integral) to their reading practices. A reading depends on the particularities of the start condition of interpretation. The decision about which features of any text come to the fore will vary from reader to reader, even from reading to reading by the same person.</p>
<p>When McGann and I began conceptualizing Ivanhoe, we were trying to take on this challenge and create a game of interpretation. The structure of Ivanhoe was envisioned to call attention to many of these basic principles. For instance, we insisted that no text was simply self-evident or self-identical. Therefore a text had to be “called”—identified and described, justified and declared, not just “brought in” to the “discourse field” as if it (the text) were an original or self-evident, primary artifact.<sup><a href="chapter_2.xhtml#chapter2-16" id="chapter2_16" role="doc-noteref">16</a></sup> Its individual features and identity, provenance and bibliographical history, were part of what was explored. The platform was designed to support altering the texts as a deliberate act of reading. By encouraging a transformative reading, <span aria-label="60" id="pg_60" role="doc-pagebreak"/>we were emphasizing the fluid (and provocative) character of a text as an encoded and probabilistic field into which a reading intervenes. The effect of each reading was visible in the form of a “move” made in the game space. Since we believe that a reading produces a text, we felt that had to be shown. We pushed for the social production of a text to be a line of research. This was manifest through linking and situating an artifact within an open-ended network of related objects and commentary. We required that individual players self-consciously and deliberately identify a role for themselves, situating their work in an acknowledged fiction of historical circumstances. We identified the field of play as always constructed from and seen through an individual perspective. We did not allow an “outside” position to the discourse field. The scene was always seen from a point of view, or, as McGann, following the nineteenth-century poet/painter Dante Gabriel Rossetti, says, “an inner standing point.” All the work of interpretation took place in social space, that is, it was part of a game played with others.</p>
<p>Ivanhoe was deliberately conceived as a scene of interpretation.<sup><a href="chapter_2.xhtml#chapter2-17" id="chapter2_17" role="doc-noteref">17</a></sup> Its features embodied many of the fundamental tenets of our model of interpretation (critical hermeneutics). The “role” situated participants deliberately within a set of conditions and cultural identities. The log of gameplay traced the dynamic interaction among players and the effect of their “moves” on each other. The social space of the game could only be viewed from within, in a situated manner, from the point of view of a player. Knowledge of the game was therefore always partial. In sum, these were parts of a probabilistic model, but the design did not go far enough to demonstrate many of the larger implications of modeling interpretation that are described in chapter 5 of this book and the appendix.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Modeling interpretation: Implications and work ahead</samp></h2>
<p class="noindent">Theories of interpretation have shifted dramatically in the twentieth and twenty-first centuries, as per the opening note about Bruns’s characterization of the “Copernican revolution” in critical work. This created a dynamic model of the interpretative (hermeneutic) activity as <i>understanding</i>. These approaches are reliant on notions of the materiality of a text—the specific properties of its language, formats, even instantiation in print or digital form, and so on. These properties are subject to transformation, but so are concepts of materiality, which is often reduced to the idea of <span aria-label="61" id="pg_61" role="doc-pagebreak"/>literal, physical, properties. Recent attention to materiality within a context of new materialisms has posited some agency to material artifacts, rather than continuing the long tradition of seeing them as static, formal, physical things. In addition to formal (organization and structure) and forensic (physically evident) notions of materiality, and the often too-literal approach to media specificity, the performative dimension of materiality needs to be taken into account. This has real implications for the ways artifacts, texts, and objects are remediated into digital environments. The performative conception of materiality is premised on the idea that we do not ask what an artifact <i>is,</i> but instead, what it <i>does.</i> At the formal level, a work is a set of encoded instructions for reading, viewing, listening, or experiencing. In a performative approach, the cognitive capacities of the reader make the work through an encounter. The humanist inventory of critical methods appropriate to such analysis is long and rich. Textual studies in the traditional and more radical modes, from close reading and new criticism through deconstruction and poststructural play, come into account. Cultural studies has its role here as well, introducing the decentering that shifts the ground from under the certainties of a single worldview, faceting any object of study along lines of inquiry that relativize judgments and values. Exposing the ideological assumptions of digital materialities and the strategies on which they claim and gain cultural authority is essential. The performance of a work provoked by a material substrate is always situated within historical and cultural circumstances and particulars and expresses cultural values (ideology) at every level of production, consumption, implementation, and design. Even the materials are understood within cultural frameworks and signify accordingly.<sup><a href="chapter_2.xhtml#chapter2-18" id="chapter2_18" role="doc-noteref">18</a></sup></p>
<p>Engagement with digital media changes when we conceptualize our projects and problems in accord with these critical tenets. Not only is our view of digital objects changed, but we see the possibilities for using the digital environment to take apart the “is-ness” of things. We can shift from an entity-based to an event-based conception of media and demonstrate the radically constitutive, codependent relations of complexity we overlook when we mistake a web of contingencies for a static, fixed, object of intellectual thought. Putting theoretical interpretation into dialogue with digital technology, we engage the opportunity for exposing the very processes by which reification takes place. Again, the ethical issues are glaring and present since these objectifying conditions mask production. We constitute <span aria-label="62" id="pg_62" role="doc-pagebreak"/>our objects of knowledge through the acts of interpretation that pretend to be observations of what already is. Perversely, the very act of putting humanists into digital projects seemed to bracket critical thinking from the design process (and take design out of the critical process). Here is where the challenge lies—not merely in critical analysis for the benefits of insight, but for the rethinking of design premises. How to bring these conceptions of materiality into the design of digital humanities projects? The answer is not to reinvent humanities theory, or critical epistemology, but to call it back into play in the design process.</p>
<p>As I have said repeatedly, the distinguishing feature of the humanities is a commitment to interpretation as a form of knowing. That knowing is staged as an informed encounter, an event, rather than a reified entity or thing. Though we refer to humanistic <i>knowledge</i> (by which we mean familiarity with a corpus of texts, events, personages, beliefs, or epistemological traditions and methodologies of various kinds), the <i>interpretative act</i> is always performed anew.</p>
<p>A few basic tenets guide humanistic approaches to interpretation. Each of these builds on and follows from the others, and each is a condensed, summary statement of complex critical histories and legacies. These realms are referenced in parenthesis, with full recognition of the problematic nature of such a list and such reductive references.</p>
<p>The first is that no text is self-identical or transparent; all texts (images, sound, etc.) are already interpretations, there are no originary or original texts (structuralism, poststructuralism, deconstruction). Second, all texts are encoded fields, provocative and probabilistic, into which an interpretation is an intervention; a text is always produced by a reading (semiotics, textual studies, critical theory). Third, any artifact of human expression is a social production, it is a snapshot or time slice through a complex network of social relations (bibliographical studies, critical theory, social history). Fourth, the materiality of an artifact is an index of that network of social relations, where materiality has to be understood as a mediation, a mediating site, in which the material instantiation is not itself to be reified as a set of entities, but conceived as a field of constitutive tensions and relations (media studies, bibliographical studies). Fifth, as a production of social relations, an artifact is a mediating site of power relations that have to be read as a codependent system of exchanges (feminism, queer theory, postcolonial theory, deconstruction, cognitive studies). Finally, we all enter into <span aria-label="63" id="pg_63" role="doc-pagebreak"/>the act of interpretation as historical subjects, masked from ourselves—we play a role with identities created through cultural conditions and personal fictions; subjectivity is registered as position (social and structural) and inflection (affect and specificity) (psychoanalysis, critical theory, poststructuralism, cognitive studies).<sup><a href="chapter_2.xhtml#chapter2-19" id="chapter2_19" role="doc-noteref">19</a></sup></p>
<p>Each of these tenets adds to the framework supporting a probabilistic approach.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Probabilistic aspects—design challenges</samp></h2>
<p class="noindent">Probabilistic and nondeterministic models of interpretation draw on radical constructivist epistemology and materialistic models of the hermeneutic encounter. How might these be modeled in a way that supports innovative visualizations? At the level of encoded protocols (operating systems, machine languages, compilers, programming) computational environments are fundamentally at odds with qualitative approaches. We can interpret these instruments and operations from a critical perspective and also build humanities content on their base, but the logic of the systems is not altered. Alan Turing’s early experiment, conducted with the assistance of Christopher Strachey, to produce a Love Letter Generator on the Mark I at Manchester University in 1953–1954, shows just how easy it is to produce apparent nonsense, even while using completely logical methods, in the same way that Lewis Carroll manipulated logic in <i>Alice in Wonderland</i> to produce coherent narratives of improbable events.<sup><a href="chapter_2.xhtml#chapter2-20" id="chapter2_20" role="doc-noteref">20</a></sup> But to incorporate the performative and probabilistic methods of the humanities into computational techniques on a systematic basis—and give them graphical expression—will take effort. The anxiety caused by trying to show ambiguity, to inscribe the situated and partial nature of user-dependent knowledge, works against the realization of these projects.<sup><a href="chapter_2.xhtml#chapter2-21" id="chapter2_21" role="doc-noteref">21</a></sup></p>
<p>The humanistic tradition is not a unified monolith, but the intellectual traditions of aesthetics, hermeneutics, and interpretative practices are core to the humanities. The insights of critical theory from a wide variety of perspectives have been brought into discussions of computational techniques, of course, but not with the notion of pushing new designs and construction. The important contributions of code studies, platform studies, and critical digital humanities are background for redesign, but rarely active contributors to its envisioning. The task is to shift from studying the <i>effects</i> of technology <span aria-label="64" id="pg_64" role="doc-pagebreak"/>(reading social media, games, narrative, digital texts, and so forth) to using humanistically informed theory to <i>design</i> the technology. This design work has to involve the rethinking of protocols, data structures and format, and information architectures, not merely surface expressions. For instance, the long-sought vision of a pluriverse (in contrast to the “universe” of a single worldview) has yet to take shape in the computational environment for humanities work or in the conventions used for its graphical expression. The notion of a pluriverse was that of multiple views of an object or issue, rather than a single one. The idea was that printed matter was almost always the expression of an individual point of view, but digital critical modes would support an infinite number of facets or perspectives. And yet, no precedents for this kind of approach have been developed. We remain locked into a single point of view, a display on a single flat screen that merely appears as a statement, effacing its mode of address to the viewer.<sup><a href="chapter_2.xhtml#chapter2-22" id="chapter2_22" role="doc-noteref">22</a></sup></p>
<p>At stake here, and in all of the projects grounded in these principles, is the claim that humanistic methods are authoritative on their own terms—as demonstrations of partial, situated, and often ambiguous user-centered (hermeneutic) models of knowledge. Modeling gender, as per the examples above, is one demonstration of creating an argument across a richly populated discourse field.<sup><a href="chapter_2.xhtml#chapter2-23" id="chapter2_23" role="doc-noteref">23</a></sup> But can the model be abstracted from the evidence, given a logical data structure, and compared, on this basis, to that of other models using the same material? The shape of the model has a connection to the structuring and ordering of argument and the way it uses evidence. A project using three hundred documents and citing them in clusters, by chronology in one part of the project, by author or theme in another, and so on through every imaginable attribute gives rise to a schematic diagram. Such evidence might be related to an external reference frame, such as a map that progresses over time, but it could merely etch its configured form against the receptive space of the screen with tokens and surrogates for each node of information, a pathway that shows the argument structure. By itself, this is an expression of a structured connection of evidence. But when contrasted with an alternative, the diagram reveals choices and decisions in sharp relief. Difference and comparison, again, are striking ways to demonstrate identity. Even when the work of such models is done intuitively, using graphical means of making interpretations, the outcome can be assessed quantitatively provided the models of data capture are sufficiently nuanced.</p>
<p><span aria-label="65" id="pg_65" role="doc-pagebreak"/>The possibilities are as unlimited as the projects. No single model of evidence and argument will be the same as any other. The weight given to evidence, degree of proximity, frequency of citation—any of these factors can be given a metric value on which to create a stable model for contrast and comparison. Such a model can also be used as a means of navigating and orienting a pathway through evidence, layering and filtering, eliding and slipping, ordering and arranging one component and another. Though the incidental shape that emerges should not necessarily be read as semantic—some features such as proximity or order might have such value but other graphic elements may be an accident of display or convenience for legibility—the form as a whole provides one view of the model of argument. Using the model to show alternative paths, choices not taken, evidence eliminated or added, the whole intellectual palimpsest of process as an emerging field stresses the probabilistic nature of interpretative work with its could-have, might-have, may-still possibilities latent within the discourse field and the work of constituting an interpretation. The ethics of inscribing point of view, location, the place from which an interpretation is made—or a statement of any kind—requires that a means of indicating such ownership be embodied in the models and their display.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Ambiguity and uncertainty</samp></h2>
<p class="noindent">Realist and mechanistic approaches depend upon the idea that phenomena are observer-independent and can be characterized as data. Data pass themselves off as mere descriptions of a priori, or given, conditions. Treating observation if it were <i>the same</i> as the phenomena observed collapses the critical distance between the phenomenal (perceivable) world and its interpretation. This undoes the basis of interpretation on which humanistic knowledge production is based. We know this. But as humanists, we have been ready and eager to suspend critical judgment in a rush to visualization. At the very least, humanists beginning to play at the intersection of statistics and graphics ought to take a detour through the substantial discussions of the sociology of knowledge and its developed critique of realist models of data gathering. At best, we need to take on the challenge of developing graphical expressions rooted in and appropriate to interpretative activity. Because realist approaches to visualization assume transparency and equivalence, as if the phenomenal world were self-evident and the apprehension of <span aria-label="66" id="pg_66" role="doc-pagebreak"/>it a mere mechanical task, they are fundamentally at odds with approaches to humanities scholarship premised on constructivist principles. I would argue that even for realist models, those that presume an observer-independent reality available to description, the methods of presenting ambiguity and uncertainty in more nuanced terms would be useful.</p>
<p>Some significant progress is being made in visualizing uncertainty in data models for geographic information systems, decision making, archaeological research and other domains, and in certain digital humanities platforms. But an important distinction needs to be clear from the outset: the task of representing ambiguity and uncertainty has to be distinguished from a second task—that of using ambiguity and uncertainty as the basis on which a representation is constructed. The difference between putting many kinds of points on a map to show degrees of certainty by shades of color, degrees of crispness, transparency, etc., and creating a map whose basic coordinate grid is constructed as an effect of these ambiguities is profoundly significant. In the first instance, we have a standard map with a nuanced symbol set. In the second, we create a nonstandard map that expresses the constructedness of space. Both rely on rethinking our approach to visualization and the assumptions that underpin it.<sup><a href="chapter_2.xhtml#chapter2-24" id="chapter2_24" role="doc-noteref">24</a></sup> Ambiguity and uncertainty are assertions of probabilistic conditions for interpretation as knowledge. Giving them graphic conventions is essential in extending the vocabulary of visualizations into critical hermeneutic practice.</p>
<p>The history of knowledge is the history of forms of expression of knowledge, and those forms change. What can be said, expressed, or represented in any era is distinct from that of any other, with all the caveats and reservations that attend to the study of the sequence of human intellectual events, keeping us from any assertion of progress while noting the facts of change and transformation. The historical, critical study of science is as full of discussions of this material as the study of the humanities. Thus the representation of knowledge is as crucial to its cultural force as any other facet of its production. The graphical forms of display that have come to the fore in digital humanities in the last decade are borrowed from a mechanistic approach to realism, and the common conception of data in those forms needs to be completely rethought for humanistic work. To reiterate what I said above, the sheer power of the graphical display of “information visualization” (and its novelty within a humanities community newly enthralled with the toys of data mining and display) seems, paradoxically, to have produced a momentary blindness.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><span aria-label="67" id="pg_67" role="doc-pagebreak"/><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Implications</samp></h2>
<p class="noindent">The goal of defining critical hermeneutics as a probabilistic system is to bring it into dialogue with graphical methods that show the workings of interpretation as a culturally situated process. The simple act of contrast is part of this approach. The work of hermeneutics cannot be codified into a set of tools, only a collection of principles whose values and beliefs guide the design and implementation of environments for this work.</p>
<p>Visualization is a critical aspect of epistemology. Too often, visualization is a representation (much mediated and remediated) passing itself off as a presentation (mere statement). Thus visualizations are often assertions or arguments that pass as declarations or statements when they could be calling attention instead to their rhetorical qualities. A visualization is a representation because of the stages of remove from any phenomenon from which capta were derived. The lifecycle of its production—parameterization, modeling, quantification, sampling, etc. and the processing into formal expression—is concealed. But a caveat here: A representation is a surrogate, a stand-in, which may or may not (need not) have qualities and characteristics of what it represents. On another level, it is not always a representation. No image is able to be equivalent to what it represents. It always has features of its construction, and it serves as a document/expression of a method of encounter between a human consciousness and a phenomenon.</p>
<p>Questions of aesthetics do not go away in these discussions. For one thing, in the history of human culture, some works are more interesting and engaging than others. But aesthetics play a role in the methods being articulated here, since they stand as the very possibility of transformation. Aesthetic imagination offers modes of thinking, seeing, understanding. Aesthetic modes articulate the distinction between directed attention and generative attention, between the didactic and the experiential. The probabilistic model of critical hermeneutics is not a “freeplay” of interpretative work, but a constrained set of possibilities. By their very exceptionalism, aesthetic objects perform important cultural work. McGann, again, states “that a literary work codes a set of instructions for how it should be read. Unlike machine and program codes, however, these codes decipher to an indeterminate number of precise outcomes. They represent exactly what Jarry called ‘a science of exceptions’.”<sup><a href="chapter_2.xhtml#chapter2-25" id="chapter2_25" role="doc-noteref">25</a></sup> This apt observation can be supported by the host of variations in interpretation produced by <span aria-label="68" id="pg_68" role="doc-pagebreak"/>literary works—or by policy documents, or MRIs, or gestures in a public speech when these are subject to interpretation. But data visualizations have depended on impoverished methods to engage with these complex approaches. The sophisticated capacities of eye and image need to be put in the service of knowledge and models of interpretation. Current practices of reductive graphs and standard charts have debased interpretation, made it simplistic and literal. We know better and can do more.</p>
<p>In a nonrepresentational approach, which will form the focus of the next chapter, visualization functions as a space for interpretation and for the act of making an argument. This approach shifts attention onto the argument structures. If current visualizations are inadequate, which they are, it is partly because the models they present are linear, mechanistic, based in empirical methods. But interpretation is nonlinear, probabilistic, emergent, and stochastic. This is what makes the link to politics through an approach to cultural/social processes that is based in similar insights about cultural phenomena. Cultural phenomena are not linear. Politics are driven by affective forces, not reason. Reason, as a construct, emerges from Enlightenment thinking that was wedded to empirical methods. Reason provided the foundation of the modern democratic project—and of Newton’s classical physics. But Reason, especially in reified form, was only a construct, not an operative force. Calculating the probabilistic relation of understanding to the unfolding conditions of the real (however imagined) will require better methods of visualization and analysis. This work has to be informed by probabilistic hermeneutic methods if its complexities and contradictions are to be engaged and supported. The tools we develop for analysis of the cultural record will work in the present as well as for materials of the past. If ever there were a moment when we are in need of humanistic methods to expose the workings of ideology, it is now, when the forms of mediated expression work through the social imaginary with potent force using computational techniques and visual methods as instruments of authority. We always need to be attentive to whose interests are served by these techniques and their capacity to conceal the processes by which they work. Visualizations conceived within a humanistic framework may have potential in the social and political spheres as well as the literary ones.</p>
</section>
<section epub:type="rearnotes" role="doc-endnotes">
<h1 class="BMH1"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Notes</samp></h1>
<ol class="footnotes">
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_1" id="chapter2-1">1</a></span>. Gerald Bruns, <i>Hermeneutics: Ancient and Modern</i> (New Haven: Yale University Press, 1992).</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_2" id="chapter2-2">2</a></span>. Ibid., “Introduction.”</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_3" id="chapter2-3">3</a></span>. Jerome McGann, “Texts in N-Dimensions,” <i>Text Technology</i> 12, no. 2 (2003), 10, <a href="http://texttechnology.mcmaster.ca/pdf/vol12_2_02.pdf">http://texttechnology.mcmaster.ca/pdf/vol12_2_02.pdf</a>; McGann, <i>The Scholar’s Art: Literary Studies in a Managed World</i> (Chicago: University of Chicago Press, 2006); and the earlier work, <i>Radiant Textuality: Literature after the World Wide Web</i> (New York: Palgrave, 2001), though it does not have any discussion of Ivanhoe.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_4" id="chapter2-4">4</a></span>. Canadian poet-theorists Steve McCaffery, Christian B<span class="dcrit">ö</span>k, Darren Wershler, and the members of the Coll<span class="dcrit">è</span>ge de Pataphysique, among others, have also used Jarry’s work as a foundation for critical practice.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_5" id="chapter2-5">5</a></span>. SpecLab was a loose association of individuals of whom some were graduate students at the University of Virginia paid for their research, one was a paid professional, and others were faculty colleagues, but it was never a formal “lab” in <span aria-label="184" id="pg_184" role="doc-pagebreak"/>any institutional sense. Among the individuals who worked with us were Bethany Nowviskie, Andrea Laue, Nick Laiacona, Worthy Martin, Geoffrey Rockwell, Steve Ramsay, and others. Credit goes to all of them in varying degrees for their contributions to the projects McGann and I envisioned.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_6" id="chapter2-6">6</a></span>. Quite a few versions of the history of early digital humanities exist, but Susan Hockey, “The History of Humanities Computing,” is a reliable introduction: <a href="http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&amp;chunk.id=ss1-2-1&amp;toc.depth=1&amp;toc.id=ss1-2-1&amp;brand=default">http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&amp;chunk.id=ss1-2-1&amp;toc.depth=1&amp;toc.id=ss1-2-1&amp;brand=default</a>. Her <i>Electronic Texts in the Humanities: Principles and Practice</i> (Oxford: Oxford University Press, 2001) is still a useful reference work.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_7" id="chapter2-7">7</a></span>. The problem of overlapping hierarchies was a major point of conversation in the 1990s and early 2000s. The question of whether documents, particularly literary and aesthetic ones, were themselves “ordered hierarchies of content objects” or merely being forced into conformance with such a model for the purposes of being “marked-up” with XML tags, produced raging debates. The arguments against the OHCO thesis are so evidently correct that it now seems odd to imagine there was such resistance, but the requirement for markup to follow nested hierarchies put much at stake in the discussion—especially as no evident solution could be created to deal with even such simple conflicts as the fact that a poem might begin and end on different pages while being constituted as a single work, or that the figurative or metaphoric use of a word in a poem would link it to multiple hierarchies that were not nested in the least. These simple examples are so fundamental to working with literary documents that the idea that scholars shrugged, gave in, and used an OCHO model now seems inconceivable.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_8" id="chapter2-8">8</a></span>. The discussion here draws on Johanna Drucker, “Humanistic Theory and Digital Scholarship,” in Matthew Gold and Lauren Klein, eds., <i>Debates in Digital Humanities</i> (Minneapolis: University of Minnesota Press, 2012), 88–89.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_9" id="chapter2-9">9</a></span>. See Andrew Robichaud and Cameron Blevins, “Tooling Up for Digital Humanities,” Stanford University, for a good introduction to many of these topics and tools for their implementation: <a href="http://toolingup.stanford.edu/?page_id=201">http://toolingup.stanford.edu/?page_id=201</a>.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_10" id="chapter2-10">10</a></span>. Johanna Drucker, “Humanities Approaches to Graphical Display,” <i>DHQ</i> (<i>Digital Humanities Quarterly</i>) 5, no. 1 (March 2011). This piece contains a formulation of the “data” and “capta” distinction applied to visualization. Bits of that article are paraphrased throughout.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_11" id="chapter2-11">11</a></span>. McGann, “Texts in N-dimensions,” 18. The reference is to Fish’s essay “Is There a Text in This Class?” in which Fish examines that question after it is posed to him by a prospective student. Fish reflects on all of the many contextual frames required to actually “read” the apparently simple question and respond to it appropriately.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_12" id="chapter2-12">12</a></span>. A useful overview is Marianne Freiberger, “A Ridiculously Short Introduction to Some Very Basic Quantum Mechanics,” <span class="symb">+</span><i>Plus Magazine</i>, May 2016, <a href="https://plus.maths.org/content/ridiculously-brief-introduction-quantum-mechanics">https://plus.maths.org/content/ridiculously-brief-introduction-quantum-mechanics</a>.</p></li>
<li role="doc-endnote"><p class="endnote"><span aria-label="185" id="pg_185" role="doc-pagebreak"/><span class="en_tx"><a href="chapter_2.xhtml#chapter2_13" id="chapter2-13">13</a></span>. Ernst von Glasersfeld, <i>Radical Constructivism: A Way of Knowing and Learning</i> (Washington, DC: Falmer Press, 1995), is a key text here. In addition, work by Humberto Maturana and Francisco G. Varela, <i>Autopoiesis and Cognition: The Realization of the Living</i> (Dordrecht: D. Reidel, 1980) and <i>The Tree of Knowledge: The Biological Roots of Human Understanding</i> (Boston: New Science Library, 1987).</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_14" id="chapter2-14">14</a></span>. Laura Mandell, “Gendering Digital Literary History: What Counts for Digital Humanities,” in Susan Schreibman, Ray Siemens, and John Unsworth, eds., <i>A New Companion to Digital Humanities</i> (Chichester, UK: Wiley Blackwell, 2016).</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_15" id="chapter2-15">15</a></span>. As mentioned above, the team included Bethany Nowviskie, Nick Laiacona, and others who helped implement and test the design features and create the functional prototype. See Johanna Drucker, “Designing Ivanhoe,” <i>Text Technology</i>, no. 2 (2003), <a href="http://www.t5d.com/tt/2004/pdf/vol12_2_03.pdf">http://www.t5d.com/tt/2004/pdf/vol12_2_03.pdf</a>.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_16" id="chapter2-16">16</a></span>. Ibid.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_17" id="chapter2-17">17</a></span>. Ibid.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_18" id="chapter2-18">18</a></span>. The discussion here draws on Johanna Drucker, “Performative Materiality and Theoretical Approaches to Interface,” <i>DHQ</i> (<i>Digital Humanities Quarterly</i>) 7, no. 1 (Summer 2013), <a href="http://www.digitalhumanities.org/dhq/vol/7/1/000143/000143.html">http://www.digitalhumanities.org/dhq/vol/7/1/000143/000143.html</a>, and Drucker, “Humanities Approaches to Graphical Display.”</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_19" id="chapter2-19">19</a></span>. Again, the phrases here and basic principles rhyme with those of McGann, “Texts in N-Dimensions,” for obvious reasons. Our work was completely meshed in the conversations about Ivanhoe and our vocabulary overlapped.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_20" id="chapter2-20">20</a></span>. An excellent introduction to these principles is Philip Yaffe, “Making Sense of Nonsense: Writing Advice from Lewis Carroll and the Jabberwocky,” <i>Ubiquity</i>, May 2008, <a href="https://ubiquity.acm.org/article.cfm?id=1386855">https://ubiquity.acm.org/article.cfm?id=1386855</a>; for more on Turing and the Love Letter Generator, see Noah Wardrip-Fruin, “Digital Media Archaeologies: Interpreting Computational Processes,” in Erkki Huhtamo and Jussi Parikka, eds., <i>Media Archaeology: Approaches, Applications, and Implications</i> (Berkeley: University of California Press, 2011), 302–322; also Homay Kay, “Alan Turing’s Automated Love Letter Generator,” “<a href="https://dukeupress.wordpress.com/2014/12/01/4701/">https://dukeupress.wordpress.com/2014/12/01/4701/</a>.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_21" id="chapter2-21">21</a></span>. The discussion here draws on Drucker, “Humanistic Theory and Digital Scholarship,” 86–87.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_22" id="chapter2-22">22</a></span>. Patrik Svensson, in his experiments at HumLab and after, has frequently worked with multiple screens and sites to avoid this kind of monocular perspective.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_23" id="chapter2-23">23</a></span>. Mandell, “Gendering Digital Literary History.”</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_24" id="chapter2-24">24</a></span>. See the discussion in Drucker, “Humanities Approaches to Graphical Display.”</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="chapter_2.xhtml#chapter2_25" id="chapter2-25">25</a></span>. McGann, “Texts in N-Dimensions,” 10.</p></li>
</ol>
</section>
</section>
</div>
</body>
</html>