<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en" xml:lang="en">
<head>
<title>Framework: Creating the Right Tools and Platforms</title>
<meta content="text/html; charset=utf-8" http-equiv="default-style"/>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:ab716dc3-7c3d-4fc9-9c55-4cc6a0edf9ef" name="Adept.expected.resource"/>
</head>
<body epub:type="frontmatter">
<div class="body">
<p class="sp"> </p>
<section epub:type="frontmatter">
<header>
<h1 class="chapter-title"><span aria-label="1" id="pg_1" role="doc-pagebreak"/><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Framework</samp><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">: Creating the Right Tools and Platforms</samp></h1>
</header>
<p class="noindent">In the several decades since humanists have taken up computational tools, they have borrowed many techniques from other fields. This has included the appropriation of visualization methods to create charts, graphs, diagrams, maps, and other graphic displays of information. But are these visualizations actually adequate for the interpretative approach that distinguishes much of the work in the humanities? To answer that question, we have to define the features of that interpretative work and identify the ways its assumptions and premises are distinct from those of other research methods. We also have to ask what kinds of attitudes toward knowledge can be expressed with current conventions of visualization and consider alternatives. If, as will be argued here, the activity of <i>modeling interpretation</i> is fundamentally at odds with current methods of <i>information visualization</i> (data display), then how would such interpretative work be structured within a computational environment and a user interface? The challenges are many.</p>
<p>We can start by considering what is being referred to in the phrase <i>information visualization</i> and how it contrasts with that of <i>modeling interpretation</i>. The standard approach to information visualization is to generate a graphic from live or static data. The process is relatively straightforward. A set of quantitative values is charted on a grid, plane, or space governed by a regular, standard metric. Columns, bars, pie charts, points, lines, network diagrams, and so on are all produced in this way. The interpretative work of shaping the data disappears from view in the final result. The image displayed on screen, in print, or through other output devices appears as a statement of fact. The interpretative dimensions of the activity that shaped the data are rendered invisible, not so much concealed as simply missing from view, absent without a trace.</p>
<p><span aria-label="2" id="pg_2" role="doc-pagebreak"/>Any statistician or quantitative analyst will freely admit to the interpretative aspects of data creation (choices about sample size, parameterization, and other aspects of statistical and quantitative manipulations).<sup><a href="framework.xhtml#fm5-1" id="fm5_1" role="doc-noteref">1</a></sup> But the idea that the graphic display is a <i>presentation of the data</i> stands unquestioned. No additional consideration is necessary. The image is considered to be the data expressed in graphic form. The goal is to find the “best” and “clearest” display of data, and the correlation between the data set and the presentation is assumed to be direct, a matter of equivalence.<sup><a href="framework.xhtml#fm5-2" id="fm5_2" role="doc-noteref">2</a></sup> Visualizations are not characterized, for instance, as remediations (translations of documents or objects from analog to digital format). They are not described as the outcome of interpretative parameters (methods of quantification) that translate numerical values into graphical ones. These methods of translation use conventions that may or may not be logically consistent with the display of quantitative information (the scale or measure may have been altered). And often, the process of data creation produces results that are far from the original phenomenon from which it was extracted. But these processes of transformation—from phenomenon to data and then to display—are rarely documented or noted. Instead, the output is simply called a “data visualization,” as if the process were unproblematic and as if the data and visualization were one and the same. In general, this is a one-way process, with visualizations generated from (that is, <i>after</i> the creation of) a data set. Sometimes values can be added to the underlying data through an interface (even the language used here emphasizes the notion that data exist first, as a primary layer, to which any modification is a later addition or layer). The data <i>structure</i> is almost never created or modified through direct manipulation of the graphical features in this situation—only the most superficial features of display (such as color or use of symbols) can be changed.</p>
<p>The idea of <i>modeling interpretation</i>, proposed here, offers a clear contrast to these practices of data visualization. The idea of <i>modeling</i> implies that a graphical expression serves as a primary mode of knowledge production, not a secondary expression of preexisting data. This suggests that a graphical expression might be used to create and/or show the features of a model of interpretative activity. This modeling approach changes the unidirectional relationship between data and display and turns the process into a two-way exchange. Thus, an act of creating an inflection, adding an attribute, or making a change in a graph would feed back into the structured data. This would create data and could also intervene in the formal structures of the <span aria-label="3" id="pg_3" role="doc-pagebreak"/>data, even changing its structure. But modeling interpretation also calls for different, distinct graphical forms capable of expressing ambiguity, contradiction, nuance, change, and other aspects of critical consideration. This requires forms and formats that do not conform to the standards of regular metrics on coordinate grids governed by a standard Cartesian system.<sup><a href="framework.xhtml#fm5-3" id="fm5_3" role="doc-noteref">3</a></sup> If we are to model interpretation—instead of displaying information—we have to imagine how this would <i>look</i>. We also have to reflect on whether we are creating an environment for <i>representing</i> interpretation or <i>creating</i> it.</p>
<p>This book addresses these issues from within a humanist perspective informed by interpretative practices (critical hermeneutics) and constructivist approaches to epistemology.<sup><a href="framework.xhtml#fm5-4" id="fm5_4" role="doc-noteref">4</a></sup> Constructivist approaches emphasize the idea that knowledge arises from experience and is not a simple perception of a preexisting world. The constructivist position takes as a premise that objects of attention—whether perceived, read, viewed, experienced—are made through acts of interpretation. (For instance, a fairy tale may exist as a text, but a child hearing this text may experience it as frightening while an adult may find it charming and nostalgic.) In this view, every text (image, artifact, score, etc.) is produced anew in a reading. Every reading proceeds from an attempt at <i>understanding</i> through which weave the inevitable and persistent questions of <i>how</i> we know what we know.<sup><a href="framework.xhtml#fm5-5" id="fm5_5" role="doc-noteref">5</a></sup> It further suggests that the reader (or reading subject) is changed by every encounter with the text. This reader not only creates a text through the negotiation of a self and a work within a historical horizon of knowledge and condition of understanding, but is also affected by it. We don’t read a text literally or mechanically. We interpret it as we read. As a work or text is produced by reading, this experience also affects the reader. Text and reader exist in a relation of reciprocity.</p>
<p>This characterization of reading suggests a codependency that is not merely limited to textual production, but to the production of the reader in the sense suggested in theories of what is termed <i>enunciation</i>. In this theoretical framework, which comes from linguistics, the individual reader is produced as a <i>subject</i> of a work. Such a subject is spoken, situated, positioned by the work. This effect has implications for the impact of texts within a cultural frame. The approach shifts the ground of a reading experience from that of isolated individual attention to one of collective effect across historical time and a specific space. Just as the architecture of a church <i>positions</i> individuals in relation to its structures and rituals, so a text positions readers within its frameworks. Issues of racial bias, gender <span aria-label="4" id="pg_4" role="doc-pagebreak"/>inequity, power relations, and legacies of colonial practice across categories of identity are structured into the textual systems. The term <i>enunciation</i> describes this structuring relationship as it is produced in language. But these concepts also apply to visual systems, as we shall see.</p>
<p>To this familiar formulation of interpretation (or critical hermeneutics), this book adds yet another dimension: the probabilistic nature of reading practices. This approach suggests that readings are interventions within the field of provocation provided by the text. Here, a text (or work of any kind) is considered a provocation, a field of potential or possibility, in which a reading or interpretation is an intervention. The act of intervention creates a unique reading in the provocative field of the work depending on the reader. The range of probability of a reading fits the normal bell curve in many instances—readings of a work will tend to cluster around a consensual understanding.<sup><a href="framework.xhtml#fm5-6" id="fm5_6" role="doc-noteref">6</a></sup> When described this way, the expectation is that the range of readings will follow a normative distribution in line with a fairly mechanistic probability. But a more radical approach to probability and interpretation suggests that the relationship between acts of reading and textual artifacts is always nondeterministic. The act of reading is an intervention in a field of provocation (the text, work, encoded experience). The outcome can be compared to the flattening of a wave function in physics, something that cannot be predicted with certainty. Readings produce a text/work in accord with the specifics of start conditions whose variables cannot be fully described or accounted for in all of their particulars.<sup><a href="framework.xhtml#fm5-7" id="fm5_7" role="doc-noteref">7</a></sup> This probabilistic model of reading will be teased out in more detail in the chapters ahead, along with attention to the issue of <i>particularity</i> in its specific relevance to visual forms of knowledge production. The probabilistic model of interpretation described here has implications for visualizations as well as for the data to which they are linked.</p>
<p>Probabilistic models of interpretation are arguments about how knowledge is produced. Thus, when dealing with the themes that will populate this book, such as the variations in systems for constructing chronological time, or for marking historical periods, or for gauging the dimensions of space or narrating encounters of discovery, the problems of modeling the acts of knowing that are central to these themes are considered at least as important as the substance of what is known. In other words, our challenge is not simply to model <i>what</i> we know, but <i>how</i>—and to recognize that the <i>what</i> is always constituted as an effect of the <i>how</i>. The task of modeling <span aria-label="5" id="pg_5" role="doc-pagebreak"/>interpretation begins with these premises. We also have to recognize that the work of display from empiricist, positivist, and mechanistic perspectives is already well served by existing conventions. These have come in large part from the natural sciences and social sciences. In those domains, a minute is always the same, a space can be measured with a standard rule, a narrative unfolds in a linear sequence, and representations are surrogates that stand in stable relation to whatever they represent. By contrast, in a user-dependent hermeneutic approach, the dimensions of time and space are influenced by factors of experience, while any understanding of a text is always produced from a position marked by historical, cultural, and individual factors.</p>
<p>User-dependent knowledge is interpretative, partial, and situated. Therefore, empirical modes of graphic display of information are as unsuited to modeling interpretation as a thermometer is for measuring the warmth of a human emotion or the strength of an embrace. We cannot measure the many (experiential, intellectual, critical) dimensions of knowledge as interpretation with the same tools as those we bring to the study of those phenomena we imagine to be outside of ourselves. In fact, the conclusion I hope to defend through the arguments made here is the contrary: that notions of externalized, user-<i>in</i>dependent knowledge are impossible within the critical epistemological approach of the humanities. Further, we might even suggest that this (critical hermeneutic) model has much to offer to fields outside the humanities—and is one of the major contributions the humanities can make to debates about the ideological and affective character of knowledge as <i>knowing</i>. The emphasis on <i>knowing</i> over <i>knowledge</i> substitutes processes (and ongoing negotiations between the subject and object) for a mechanistic model of perception (in which the world is assumed to simply become reified in representations of things).</p>
<p>The emphasis on <i>knowing</i> situates the interpretative process in a user-dependent model. The distinction between user-independent and user-dependent models of knowledge distinguishes empirical (mechanistic) from interpretative (hermeneutic) approaches. Within the tenets of modern physics, this line blurs. Since Albert Einstein, Werner Heisenberg, and other physicists working in the early twentieth century began to grapple with the mutable conditions of space-time, the concepts of uncertainty and probability, and the relation between observation and outcome have become part of the study of natural phenomena, particularly at certain scales (such as that of subatomic particles). The humanities have lagged behind. This <span aria-label="6" id="pg_6" role="doc-pagebreak"/>is changing within the frameworks of probabilistic interpretation (critical hermeneutics), constructivist approaches to knowledge (radical epistemology), and more recently, new materialisms (theories in which the material world has some agency). Humanities scholars have grappled with some of the features of what can be considered <i>codependence</i> (between subject and object, reader and text) in the fundamental processes of knowledge production. But these concepts have not been brought to bear on visual display and graphical expressions. Even in textual humanities, a persistent strain of positivism—suggesting that a text has a meaning that can be fixed and grasped—continues to push against the tenets of constructivism, especially in digital projects. The approach to modeling interpretation outlined here builds on established principles of hermeneutic understanding taken from the critical tradition, but makes its contribution through a focus on the specific problems of graphical expression and production of interpretative practices within the probabilistic frameworks just outlined.</p>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Outline of this book</samp></h2>
<p class="noindent">This book is structured in five parts. Chapter 1, “Visual Knowledge (or <i>Graphesis</i>),” contains a discussion of visual knowledge production (epistemology) and the specific properties of graphical approaches within a digital environment. Here the questions posed have to do with the relation between inscription and notation, between producing and recording knowledge, and also, with the specificity of visual expressions required for interpretative work. This chapter looks at various theoretical understandings of visual images and their relation to knowledge and asks how the specifics of the graphical are to be engaged directly as a primary means of knowledge production for digital humanities. What kind of knowledge belongs to the visual and how can computational environments support this activity within a critical interpretative (hermeneutic) framework? How do graphical methods produce and inscribe knowledge differently than textual, numeric, or other approaches? The chapter draws on work from aesthetics, critical theory, and formal study of graphical systems and addresses these within the specific framework of computational and digital activity as they apply to digital humanities.</p>
<p>Chapter 2, “Interpretation as Probabilistic,” is concerned with defining the specific parameters of the humanistic methods invoked here. It describes what is meant by user-centered conditions of interpretation, how <span aria-label="7" id="pg_7" role="doc-pagebreak"/>they are enacted, and what purpose is served by outlining a claim for their cultural authority as well as their practical use. Here the tenets of critical interpretation (hermeneutics) meet the actualities of research problems and the discursive heterogeneity of the cultural record. This chapter outlines the basic properties of a partial and situated approach to knowledge as interpretation and its role within the digital humanities. It expands the notion of probabilistic interpretation as a crucial aspect of scholarly work within digital environments. This approach is presented with an argument for its political impact and value, as well as its critical purchase on expressions and instantiations of the cultural record when these are read, analyzed, and studied within the mediated and remediated conditions of digital formats.</p>
<p>Chapter 3, “Graphic Arguments,” proposes an approach to graphical methods of interpretation grounded in a nonrepresentational understanding of visualization (beginning with a discussion of what <i>nonrepresentational</i> images are). This chapter discusses argument forms and graphical rhetoric as primary tools of and for interpretation. It also proposes a set of conventions for the creation of argument structures and for their direct encoding within data structures. It also addresses the challenge of taking informal approaches, some of which resist the very terms of formal languages and expressions, into structured data. But the more immediate concern of this project is with developing the conventions according to which the rhetorical force of interpretative work can be enacted using graphical methods that link directly to the production of data and code.</p>
<p>Chapter 4, “Interface and Enunciation,” takes a step back from the direct work of interpretation (of texts, images, etc.) to address the problems of the enunciative (subject-producing) framework within which digital work takes place and the concealment enacted by interface. The analysis of interface as an enunciative system, and of information as an aspect of that system, is central to understanding the ways in which a reader or scholar’s position as a subject is created in screen, device, platform, and projection environments. This chapter also addresses the problems of the lifecycle of data production and its obfuscation. The questions of how data production can be visualized, and how the interpretative modeling of what passes for information can be exposed, are addressed here as part of an attention to the larger problems of the function, performance, and role of the enunciative apparatus of information systems. The link between enunciative systems and the enactment of power relations invokes interface as a major site and <span aria-label="8" id="pg_8" role="doc-pagebreak"/>instrument of ideology, along with the need to create visual techniques for showing its workings.</p>
<p>Finally, chapter 5, “The Projects in Modeling Interpretation,” presents a series of projects that incorporate these theoretical formulations. They are all standard problems in visualization for the humanities—time/temporality, space/spatial relations, data analysis, and so on—but the investigation is posed in terms of innovative graphical systems informed by probabilistic critical hermeneutics. I carried on the preliminary work on each of these problems in specific research projects beginning in the early 2000s. These include two approaches to timelines: (1) temporal modeling (the system for creating analyses of the complex temporal relations in humanistic documents and corpora, experiential records, and other materials), and (2) heterochronologies (the modeling of systems of time-keeping according to specific and irreconcilable metrics, or comparative ontologies). The problems also include spatial modeling (spatial presentations created in accord with various factors, in a nonrepresentational approach to experiential, variable-metric mapping). Another problem is network inflection (modeling relations among elements in a standard edge-node formulation as node-edges that change across a set of variables that inflect the node-edge as a codependent entity). The final problem engages with argument structures and discourse fields (the shape of argument and its relation to evidence, combining automated techniques such as topic modeling with user-created argument structures); enunciation and the exposure of the hermeneutic lifecycle of data production (conventions for showing the modeling process of data and identifying subject positions within the apparatus of the interface). This chapter finishes with a final brief sketch of discovery tools as an additional interface into which modeling can be worked.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Intellectual foundations</samp></h2>
<p class="noindent">The projects in this book, and formulations on which they are based, have developed over a period of two decades. Concerns about visual epistemology and its relation to textual and graphical forms have been integral to my work on the alphabet, visual poetics, book forms and formats, the graphic arts, and related fields for much longer.<sup><a href="framework.xhtml#fm5-8" id="fm5_8" role="doc-noteref">8</a></sup> I am not a philosopher by training. I am a scholar-practitioner educated in a succession of critical approaches. My intellectual trajectory merged structuralism, poststructuralism, critical aesthetics, <span aria-label="9" id="pg_9" role="doc-pagebreak"/>theories of enunciation and subject formation in linguistics, film, psychoanalysis, and feminism with concepts of constructivist epistemology. This education occurred long before recent developments in what is termed <i>new materialism</i> had imagined its version of probabilistic encounters as a codependent relation between subject and object in a way that helps dissolve that binary. This combination of theoretical positions informs my work with deep convictions about <i>the primacy of visual knowledge within a probabilistic codependent framework rooted in humanistic interpretation (and critical hermeneutics).</i> These have not, as far as I know, been expounded elsewhere in relation to information visualizations, though elements of this approach can be found in cartography and arguments about relational space.<sup><a href="framework.xhtml#fm5-9" id="fm5_9" role="doc-noteref">9</a></sup></p>
<p>The reader doesn’t need to know this intellectual history, but it is helpful, perhaps, to recognize that the work presented has had a long gestation period and was presented in multiple forms and many venues as articles, book chapters, talks, and funded research work in the last two decades. Pulling this together now, as the field of digital humanities continues to expand, but still without claiming the full cultural authority of critical interpretative work that is its core concern, makes my argument feel all the more urgent. Twenty years ago, the conceptual frameworks on which these projects are based seemed out of reach of implementation. Now, they seem ripe for realization and adoption. The obstacles to realization are not computational—they never were—but cultural. The resistance to the uncertainties and ambiguities of probabilistic hermeneutics introduces discomfort in many circumstances. But the desire for certainty of expression, for visualizations that are strictly representational, has a psychological rather than intellectual foundation. The challenge is to shift that ground and make the force of argument, the rhetorical power of intellection, the heterogeneous structures of historical moments, cultural viewpoints, and individual judgment into a systematic and legible practice with appropriate tools and platforms. The timeline of these works also aligns with an increased interest in the decolonization of knowledge and all that this implies. This includes challenging assumptions long built into intellectual frameworks, such as those that privilege empirical methods and positivist frameworks. The situated conditions of knowledge production are politically charged, not just intellectually inflected. Alternatives to rationalized approaches are endemic to critical thinking, particularly in a skeptical vein, and are essential to rethinking visualization in its interpretative role.</p>
<p><span aria-label="10" id="pg_10" role="doc-pagebreak"/>Another obstacle remains—the longstanding distrust of visual methods as primary modes of epistemological work. The humanities have long depended upon and been concerned with texts. In digital and computational activities, humanists have accepted the authority of quantitative and statistical approaches, perhaps because they appear explicit and unambiguous in their presentation of information. Almost paradoxically, the humanist, sure of theoretical, intellectual ground in the realm of discourse, seems to suspend that critical discussion when using the quantitative methods on which computation operates. The implication is that critical approaches belong to <i>content</i> in digital humanities projects, rather than to methods. But of course, this is wrongheaded and limiting. I have long argued that we need to insist on the use of humanities <i>methods</i> to develop computational tools and digital platforms.</p>
<p>And visuality? A long history of suspicion attends to the role of the visual in western thought. Philosophers beginning with Plato have distrusted visual images on a whole variety of grounds—as mere imitations, pale shadows, or much worse. To model interpretation using visual forms and with novel conventions pushes against many prejudices in the humanities in western culture. When we borrowed visuals from the natural and social sciences, the humanists seem to imply, at least we knew that we were borrowing systems grounded in representational strategies that correlated quantitative values and regular metrics. They were presumed to be solid on their own empirical terms. We borrowed them the way we might, unthinkingly, borrow a ruler—without asking when, by whom, or for what purpose the standard measures of inches and feet, yards, meters, or picas came into being and how they signify discursively—and visually. But the standard metric is an ideological construction, not a natural one. Also paradoxically, visual methods of its encoding render the ideological aspects nearly invisible—while also making it difficult to envision alternatives rooted in affective and interpretative activities.</p>
<p>To engage with a system of rhetorical expressions premised on the idea that visual arguments can become structured data, that informal and ambiguous graphics can give rise to formal expressions, may stretch credibility and patience. But that is the task undertaken here, along with the justification for the effort, because the cultural authority of critical positions cannot be asserted, enacted, or demonstrated without adequate means of expression in graphical ones. The politics of knowledge production are intimately bound to the methods through which it is enacted.</p>
</section>
<section epub:type="rearnotes" role="doc-endnotes">
<h1 class="BMH1"><span aria-label="177" id="pg_177" role="doc-pagebreak"/><samp class="SANS_ITC_Stone_Sans_Std_Semibold_B_11">Notes</samp></h1>
<ol class="footnotes">
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="framework.xhtml#fm5_1" id="fm5-1">1</a></span>. Lisa Gitelman’s edited volume, <i>“Raw Data” Is an Oxymoron</i> (Cambridge, MA: MIT Press, 2013), addresses these issues from within a critical digital media studies approach, but even the most straightforward introductory text, John Dillard’s “5 Most Important Methods for Statistical Data Analysis” (n.d.) states very clearly what the pitfalls of each standard method are and introduces critical caveats on mean, standard deviation, regression, sample size, and other factors; see <a href="https://www.bigskyassociates.com/blog/bid/356764/5-Most-Important-Methods-For-Statistical-Data-Analysis">https://www.bigskyassociates.com/blog/bid/356764/5-Most-Important-Methods-For-Statistical-Data-Analysis</a> (accessed August 19, 2018).</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="framework.xhtml#fm5_2" id="fm5-2">2</a></span>. Calvin Schmid, <i>Statistical Graphics: Design Principles of Practices</i> (Brentwood, CA: John Wiley, 1983), and Stephen Few, <i>Show Me the Numbers: Designing Tables and Graphs to Enlighten</i> (El Dorado Hills, CA: Analytics Press, 2004), are two excellent sources for engagement with the basics of data visualization. Schmid’s work is much older, written when statistical graphics were all print-based, but it contains solid, useful, pragmatic, and critically focused discussion of graphic techniques. Few’s work is more recent, uses images generated entirely in Excel, and also addresses visualizations from a critical perspective. Both approach visualization from a positivist and empirical perspective without the least hint of critical, theoretical humanistic reflection.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="framework.xhtml#fm5_3" id="fm5-3">3</a></span>. The Cartesian system of <i>x</i> and <i>y</i> axes is highly rational, standardized, and conventional as a way to create an underlying grid structure for any visual images such as a graph, chart, or map.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="framework.xhtml#fm5_4" id="fm5-4">4</a></span>. The term critical hermeneutics refers to interpretative practices conceived within the tradition of Martin Heidegger and Hans-Georg Gadamer and continued into contemporary ramifications, see: Section 2.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="framework.xhtml#fm5_5" id="fm5-5">5</a></span>. Solomon Wiener, “History, Tradition, and Understanding in Gadamer’s <i>Truth and Method</i>,” <i>OR Journal</i> (April 2017), <a href="https://orintercollegiatejournal.com/2017/04/30/history-tradition-and-understanding-in-gadamers-truth-and-method1/">https://orintercollegiatejournal.com/2017/04/30/history-tradition-and-understanding-in-gadamers-truth-and-method1/</a>; and Adrian <span aria-label="178" id="pg_178" role="doc-pagebreak"/>Costache, <i>Gadamer and the Question of Understanding: Between Heidegger and Derrida</i> (Lanham, MD: Lexington Books, 2016). See review by Lawrence K. Schmidt, <a href="https://ndpr.nd.edu/news/gadamer-and-the-question-of-understanding-between-heidegger-and-derrida-2/">https://ndpr.nd.edu/news/gadamer-and-the-question-of-understanding-between-heidegger-and-derrida-2/</a>.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="framework.xhtml#fm5_6" id="fm5-6">6</a></span>. Work in the statistical analysis of reading comprehension is typical of this approach. For an example, see Cindy Brantmeier, “Statistical Procedures for Research on L2 Reading Comprehension: An Examination of ANOVA and Regression Models,” <i>Reading in a Foreign Language</i> 16, no. 2 (October 2004), <a href="http://nflrc.hawaii.edu/rfl/October2004/brantmeier/brantmeier.html">http://nflrc.hawaii.edu/rfl/October2004/brantmeier/brantmeier.html</a>, and extensive research work in <i>Reading Research Quarterly</i>.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="framework.xhtml#fm5_7" id="fm5-7">7</a></span>. Those familiar with quantum theory will recognize the Copenhagen interpretation of Niels Bohr and Werner Heisenberg in this language. My use of quantum theory is not merely metaphoric, it is meant to describe the actual workings of interpretative methods and encounters. See Jan Faye, “Copenhagen Interpretation of Quantum Mechanics,” July 24, 2014, <i>Stanford Encyclopedia of Philosophy</i>, <a href="https://plato.stanford.edu/entries/qm-copenhagen/">https://plato.stanford.edu/entries/qm-copenhagen/</a>.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="framework.xhtml#fm5_8" id="fm5-8">8</a></span>. See my <i>The Visible Word</i> (Chicago: University of Chicago Press, 1994), <i>The Alphabetic Labyrinth</i> (London: Thames and Hudson, 1994), <i>Figuring the Word</i> (New York: Granary, 1998), <i>SpecLab</i> (Chicago: University of Chicago Press, 2009), <i>Graphesis</i> (Cambridge, MA: Harvard University Press, 2014), <i>Diagrammatic Writing</i> (Eindhoven, Netherlands: Onomatopee, 2013), and many articles on related topics.</p></li>
<li role="doc-endnote"><p class="endnote"><span class="en_tx"><a href="framework.xhtml#fm5_9" id="fm5-9">9</a></span>. Concepts of the probabilistic relation of reader and text, and the idea of a text as a provocation for reading, were developed in conversation with Jerome McGann in the early 2000s in the period when we were creating the intellectual and design framework for Ivanhoe, a Game of Interpretation. The temporal modeling project preceded Ivanhoe and was designed in 2000–2001, while Ivanhoe followed in 2002–2003 as part of our SpecLab work. McGann’s commitment to a probabilistic model, however, strongly resonated with the work I had done in some of the essays in <i>Figuring the Word</i> and the creative texts of <i>Deterring Discourse</i> (New York: Druckwerk, 1993), <i>Emerging Sentience</i> (New Haven: JAB books, 2001), and <i>Prove before Laying</i> (New Haven: Druckwerk, 1997). But my debt to McGann—and his to me—is evident in the shared vocabulary and conceptual overlap between his work in “Texts in N-Dimensions and Interpretation in a New Key [Discourse and Interpretation in N-Dimensions],” written in 2002. (See <a href="https://texttechnology.humanities.mcmaster.ca/pdf/vol12_2_02.pdf">https://texttechnology.humanities.mcmaster.ca/pdf/vol12_2_02.pdf</a>.) McGann’s exposition in this piece is the most highly developed version of our thinking and surpasses my own in depth and detail. My appreciation of those conversations remains undiminished. However, missing from McGann’s otherwise rich insights was any engagement with visuality. The digital humanities were almost entirely text-based in the 1990s, with very few exceptions.</p></li>
</ol>
</section>
</section>
</div>
</body>
</html>